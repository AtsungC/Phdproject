---
title: "time_series"
output: 
  pdf_document:
      number_sections: TRUE
      toc: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,cache = TRUE)
```

# Read Time Series Date
* scan() : read data into a vector or list from the console or file
* ts() : store the data into time series format
```{r}
# alt+ctrl+i = create a new code chunk
# import data
kings <- scan("http://robjhyndman.com/tsdldata/misc/kings.dat",skip=3)
kings
# store the data in a time series object 
# ts(), frequency = 'the time interval data collected', such as 12 for montly, 4 for quarter
# start(1982,2) = the data start from 1982 second quarter
kingstimeseries <- ts(kings)
kingstimeseries
```
```{r}
# the example of advanced use 
births <- scan("http://robjhyndman.com/tsdldata/data/nybirths.dat")
births.ts <- ts(births, frequency = 12,start = c(1946,1))
knitr::kable(head(births.ts),'markdown')
```
# Plotting Time Series
* addictive model : be able to decribe seasonal data but trend
    * trend data can be transformed into log to decrease the trend effect.


```{r}
plot.ts(kingstimeseries)
```

# Decomposing Time Series
* to separat the data into its constituent components
 
## Decomposing Non-Seasonal Date
* a non-seasonal time series consists of **a trend component** and **an irregular component**
* to estimate the trend component of a non-seasonal time series that can be described using an additive model : a smoothing mothod, such as MA.
* SMA() : 
    * in "TTR" package
    * is used to smooth time series data, a simple moving average
    * specify the order of the simple moving average, n
         * to estimate the trend componet, might try smoothing the data with a simple moving average of a higher order
         * trial-and-error
```{r}
library(TTR)
kingstimeseriesSMA3 <- SMA(kingstimeseries, n=3)
plot.ts(kingstimeseriesSMA3)
```

## Decomposing Seasonal Date
* a seasonal data consists of **a trend component**, **a seasonal component** and **an irregular component**.
* to estimate the trend component and seasonal component, using additive model
    * decompose() : decompose a time series into **seasonal**, **trend** and **irregular components**, using moving averages. 
        * return a list object
```{r}
births.ts.decom <- decompose(births.ts)
plot(births.ts.decom)
```

## Seasonally Adjusting 
* if the seasonal time series can be described by an additive model, the data can be seasonally adjusted by estimating the seasonal component and substracting the estmated seasonal component from the original time series. 
* the seasonally adjusted result just contains **the trend component** and **an irregular component*
* decompose() 
```{r}
births.ts.decom.seasonallyadjusted <- births.ts - births.ts.decom$seasonal
plot(births.ts.decom.seasonallyadjusted)
```

# Forecasts using Exponential Smoothing
* being used to make short-time forecasts for time series data

## Simple Exponential Smoothing
* if the data can be described using an additice model with **constant level** and **no seasonality**
* using simple exponential smoothing to make short-time forcasts
* alpha : lies between 0 and 1
    * close to 0 : meaning little weight is placed on the most recent observations when making forecasts of future values
* HoltWinters() : simple exponential smoothing 
    * beta = FALSE ; gamma = FALSE : for Holt's exponential smoothing or Holt-Winters exponential smoothing 
    * **fitted** : the forcasts made by HoltWinters is stored in fitted
    
```{r}
rain <- scan("http://robjhyndman.com/tsdldata/hurst/precip1.dat",skip=1)
rain.ts <- ts(rain,start = 1813)
plot.ts(rain.ts)
```

```{r}
rain.ts.forcast <- HoltWinters(rain.ts,beta=FALSE,gamma=FALSE)
knitr::kable(head(rain.ts.forcast$fitted))
plot(rain.ts.forcast)
```

* the accuracy offorcasting : the sum of squared errors  
    * $SSE 

## Specifiy the inital value for the level in the HW model
* l.start 

## forecast.HoltWinters() 
* make forecast for further time point
* *forecast* package
* h : how many further time points going to make forecast
**forecast.HoltWinter() could not find in the package**
**stats::forecast.HoltWinters() is not availibale for R 4.0**
**forecast::forecast() has the same function**
* the predicted area, 80% and 95% prediction interval
```{r message=FALSE}
require('forecast')
rain.ts.forcast2 <- forecast::forecast(rain.ts.forcast, h=8)
knitr::kable(rain.ts.forcast2,'markdown')
plot(rain.ts.forcast2)
```

* if there are correlations between **forecast errors** for **successive predictions**, it is likely that the simple exponential smoothing forecasts could be improved upon by another forecasting technique. 
* forecast error : a measure of accurancy of the predictive model is SSE, sum of squared error for the **in-sample** forecast errors. it is stored in **residual** of the forecast of HoltWinters. 
* if the SSE could be improved, the predictive model should be improved as well.
* acf() : showing a correlogram of the **in-sample** forecast errors, to specifiy the max lag 
```{r}
acf(rain.ts.forcast2$residuals,lag.max = 20, na.action=na.pass)
#na.action=na.pass  to pass the NA data
```
* the lag3 is just touching the significance bounds
* **Ljung-Box text** : to test whether there is significant evidence for non-zero correlations at lags 1-20.
* Box.test() : Ljung-Box test
```{r}
Box.test(rain.ts.forcast2$residuals,lag = 20,type = 'Ljung-Box')
```
* the p-value is 0.6, so there is a little evidence of non-zero autocorrelation in the in-sample forecast error at lags1.20.
* checking the normal distribution with mean zero and constant variance : to be sure that the predictive model cannot be improved upon.

```{r}
plot.ts(rain.ts.forcast2$residuals)
```
* histogtam plotting : to check whether the forecast errors are normally distributed with mean zero.
```{r}
plotForecastError <- function(forecasterrors){
     forecasterrors <- na.omit(forecasterrors)
  # make a histogram of the forecast errors:
     mybinsize <- IQR(forecasterrors)/4
     mysd   <- sd(forecasterrors)
     mymin  <- min(forecasterrors) - mysd*5
     mymax  <- max(forecasterrors) + mysd*3
     # generate normally distributed data with mean 0 and standard deviation mysd
     mynorm <- rnorm(10000, mean=0, sd=mysd)
     mymin2 <- min(mynorm)
     mymax2 <- max(mynorm)
     if (mymin2 < mymin) { mymin <- mymin2 }
     if (mymax2 > mymax) { mymax <- mymax2 }
     # make a red histogram of the forecast errors, with the normally distributed data overlaid:
     mybins <- seq(mymin, mymax, mybinsize)
     hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
     # freq=FALSE ensures the area under the histogram = 1
     # generate normally distributed data with mean 0 and standard deviation mysd
     myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
     # plot the normal curve as a blue line on top of the histogram of forecast errors:
     points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
}
plotForecastError(rain.ts.forcast2$residuals)
```

* the Ljung-Box : little evidence of non-zero autocorrelation
* the normal distribution with mean zero
* conclusion : the simple exponential smoothing method provides an adequate predictive model.
* the 80%-95% predictive intervals are valid

# Holt's Exponential Smoothing
* the dataset with trend (increasing or decreasing) and no seasonality and can be described by an addictive model. The short-time forecasts can be made by Holt's exponential smoothing.
* Holt's exponential smooting estimate the level and slope at the current time point. 
* Smoothing : 
    * alpha, for the estimate of **the level** at the current time point. 
    * beta, for the estimate of **the slope of the trend component** at the current point
```{r}
skirts <- scan("http://robjhyndman.com/tsdldata/roberts/skirts.dat",skip=5)
skirts.ts <- ts(skirts,start = 1866)
plot.ts(skirts.ts)
```

* HoltWinters(gamma=FALSE) : Holt's exponential smoothing 
```{r}
skirts.ts.fc <- HoltWinters(skirts.ts, gamma=FALSE)
skirts.ts.fc
skirts.ts.fc$SSE
plot(skirts.ts.fc)
```
* the estimate value of alpha and beta are both high, which mean the current value of the level and of the slop of the trend component are based mostly upon very recent observations in the time series.
* the SSE is high cause the level and the slop of the time series change a lot over time
```{r}
skirts.ts.fc2 <- forecast::forecast(skirts.ts.fc,h=19,na.action=na.pass)
plot(skirts.ts.fc2)
```



# Introductory time series with R 
* ternary parts : function(n) if (n==1) 1 else n*(n-1)
* the **matrix calculation** can replace the **loop** and fasten the process
## Plots, trends, and seasonal variation
### A flying start : Air passenger bookings
```{r}
data(AirPassengers) # it is a time series data 
AP <- AirPassengers
class(AP)

start(AP);end(AP);frequency(AP)

plot(AP,ylab="Passengers(1000's)")
plot(aggregate(AP)) # clear trend feather 
summary(AP)

#layout(1:2) display graphic for multple graphs
cycle(AP) # label the frequency 1-12 to each value 

boxplot(AP~cycle(AP))
```

```{r , eval=F}
path <- "http://www.massey.ac.nz/~pscowper/ts/Maine.dat"# not found
Maine.month <- read.table(path,header=T)
class(Maine.month)
```
```{r}
path <- "http://www.massey.ac.nz/~pscowper/ts/USunemp.dat"
us.month <- read.table(path,header=T)
```

* ts- time series
* zoo- Zeileis's ordered obervations : enhance ts, providing irregular time series
* xts- eXtensible time series : enhances zoo by adding **metadata** to time-series data

* ts- time series
    * reguarly spaced time series
    * cantains **data matrix** and **time vector**
    * adding and substraction can be performed at common time point at time series
```{r}
data <- matrix(c(sin(seq(0,10,0.1)),
                 sin(seq(1,11,0.1)),
                 sin(seq(2,12,0.1)),
                 sin(seq(3,13,0.1))),
               ncol = 4)
dim(data)
thefrequencyofayear <- 12 #monthly, 52=weekly
name.col <- c('E','W','N','S')
data.ts <- ts(data,end=2020,frequency=thefrequencyofayear,names=name.col)
head(data.ts)
```
    

*zoo 
    * compatible with ts
    * handle **irregularly** spaced time series
        * ex: stock market reading on different date of a week
    * index the data 
        * time search : as.POSIXct('time')
    * more flexible and applied to other statstical methods
    * when doing time zone transfer, be careful of **BST** and **GMT**, there is a missing hour.
    
```{r}
library(zoo)
weatherData <- read.table("https://raw.githubusercontent.com/lyndadotcom/LPO_weatherdata/master/Environmental_Data_Deep_Moor_2012.txt",
                          header = TRUE, 
                          stringsAsFactors = FALSE)
head(weatherData)
dim(weatherData) # number of columns =9
zoo.weather <- as.matrix(weatherData[,-(1:2)],ncol=7) # exclude data and time columns
zoo.weather.datetime <- as.POSIXct(paste(weatherData[,1],weatherData[,2]),"%Y_%m_%d %H:%M:%S",tz='America/Los_Angeles') 
ts.zoo <- zoo(x=zoo.weather,order.by = zoo.weather.datetime)
ts.zoo[as.POSIXct('2012-01-01 ')]
```

*xts 
    * compatible with zoo
    * enhanced statistics
    * attributes
    * split data in a ragnge of time
    * can search data in a ragne
    
```{r}
library(xts)
weatherData <- read.table("https://raw.githubusercontent.com/lyndadotcom/LPO_weatherdata/master/Environmental_Data_Deep_Moor_2012.txt",
                          header = TRUE,
                          stringsAsFactors = FALSE)
zoo.weather <- as.matrix(weatherData[,-(1:2)],ncol=7) # exclude data and time columns
zoo.weather.datetime <- as.POSIXct(paste(weatherData[,1],weatherData[,2]),"%Y_%m_%d %H:%M:%S",tz='America/Los_Angeles')
xts.wt <- xts(x=zoo.weather,order.by = zoo.weather.datetime)
xts.wt.at <- xts(x = zoo.weather, 
              order.by = zoo.weather.datetime,
              favoriteAnimal = "rabbit")
xtsAttributes(xts.wt.at)
ts.wt.mo <-  splice(xts.wt,f='months')

```

* time series and the tidyverse 
    * tsibble 
    * tibbletime
        * time series data : **dont require conversion to matrix**
        * similar to ts, zoo and xts
        * similar functionality
```{r}
library(tsibble)
library(tibbletime)
library(tidyverse)
library(lubridate)
tt_weatherData <- read_table2("https://raw.githubusercontent.com/lyndadotcom/LPO_weatherdata/master/Environmental_Data_Deep_Moor_2012.txt") %>% 
  unite(datetime, date,time) %>%
  mutate(datetime = ymd_hms((datetime))) %>%
  tbl_time(index = datetime)

ts_weatherData <- read_table2("https://raw.githubusercontent.com/lyndadotcom/LPO_weatherdata/master/Environmental_Data_Deep_Moor_2012.txt") %>% 
  unite(datetime, date,time) %>%
  mutate(datetime = ymd_hms((datetime))) %>%
  as_tsibble(index = datetime, key = Air_Temp) 

```

* rolling statistics
    * roll function
* time series graphs 

